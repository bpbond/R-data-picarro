Reproducible data analysis using R
========================================================
author: Ben Bond-Lamberty
date: February 2016
font-family: 'Helvetica'

A workshop covering a bit of R basics; reproducibility; graphics and pipelines; and data summarizing and manipulation.

Purdue University


The next three hours of your life
========================================================

* Introduction: R basics and reproducible research (45 minutes; hands-on: installing the packages we'll need)
* Examining and cleaning data (45 minutes; hands-on: the `iris` dataset)
* Summarizing and manipulating data (45 minutes; hands-on: the `babynames` dataset)
* Bringing things together: working with Picarro data (45 minutes; hands-on: real-world scientific data)

Feedback: <a href="mailto:bondlamberty@pnnl">bondlamberty@pnnl.gov</a> or  [@BenBondLamberty](https://twitter.com/BenBondLamberty).


Is R the right tool for the job?
========================================================

>R has simple and obvious appeal. Through R, you can sift through complex data sets, manipulate data through sophisticated modeling functions, and create sleek graphics to represent the numbers, in just a few lines of code...R’s greatest asset is the vibrant ecosystem has developed around it: The R community is constantly adding new packages and features to its already rich function sets.
>
>-- [The 9 Best Languages For Crunching Data](http://www.fastcompany.com/3030716/the-9-best-languages-for-crunching-data)


Is R the right tool for the job?
========================================================

But it might not be. R has limitations and weaknesses:
- nontrivial learning curve; quirks; inconsistent syntax
- documentation patchy and terse
- package quality varies
- not designed for *large* datasets

There are other tools that might be better for your specific need!
- Python, C++, Hadoop, CDO/NCL, bash, ...
- Excel in _extremely_ limited circumstances


Reproducibility
========================================================
type: section


Reproducibility
========================================================

We are in the era of collaborative 'big data', but even if you work by yourself with 'little data' you have to have some skills to deal with those data.

**Most fundamentally, your results have to be reproducible.**

>Your most important collaborator is your future self. It’s important to make a workflow that you can use time and time again, and even pass on to others in such a way that you don’t have to be there to walk them through it. [Source](http://berkeleysciencereview.com/reproducible-collaborative-data-science/)

Reproducibility generally means *scripts* or *programs* tied to *open source software*.


You can't reproduce
========================================================
...what doesn't exist.

**Gozilla ate my computer!**
* backup
* ideally *continuous*

**Godzilla destroyed my office!!!!!!**
* offsite (cloud)

***

<img src="images/godzilla.jpg" width="400" />


You can't reproduce
========================================================

...what you've lost. What if you need access to a file as it existed 1, 10, or 100, or 1000 days ago?
- Incremental backups (minimum)
- Version control (better). A *repository* holds files and tracks changes

***

<img src="images/tardis.jpg" width="400" />


Version control
========================================================

**Git** (and website **GitHub**) are the most popular version control tools for use with R, and many other languages:
- version control
- sharing code with collaborators in a *repository*
- issue tracking
- public or private

***

<img src="images/github.png" width="400" />


Reproducible research example
========================================================

A typical project/paper directory for me:
```
1-download.R
2-process_data.R
3-analyze_data.R
4-make_graphs.R
logs/
output/
rawdata/
```

This directory contains *scripts* that are backed up both *locally* and *remotely*. It is under *version control*, so it's easy to track changes over time.


Reproducible research example
========================================================

<img src="images/repository.png" />


Hands-on: setting up R and RStudio
========================================================
type: prompt
incremental: false

If you're doing the exercises and problems, you'll need these
packages:
- `dplyr` - fast, flexible tool for working with data frames
- `ggplot2` - popular package for visualizing data

We'll also use this data package:
- `babynames` - names provided to the SSA 1880-2013

Finally, we'll download a [repository](https://github.com/bpbond/R-data-picarro) (collection of code and data) for this workshop.

Let's do that, and get oriented in [RStudio](https://www.rstudio.com/products/RStudio/), now.


R basics
========================================================
type: section


Things you should know: basics
========================================================

This workshop assumes you understand a few basics of R:

- What R is
- How to start and quit it
- How to get help

```{r, eval=FALSE}
# This is a comment
# Get help for the `summary` function
?summary

# Get help for an entire package
help(package = 'ggplot2')
```


Things you should know: basics
========================================================

- The idea of *objects*, *functions*, *assignment*, and *comments*

```{r}
x <- 10 + 2 # `x` is an object
log(x) # `log` is a function
```

- The idea of *data types*
```{r}
x <- 3.14     # numeric
y <- "hello"  # character
z <- TRUE     # logical
# We're going to avoid this next one
f <- factor(c("apple", "pear", "banana")) 
```


Things you should know: vectors
========================================================

- The *vector* data type

```{r}
myvector <- 1:5
myvector <- c(1, 2, 3, 4, 5)
myvector <- seq(1, 5)
myvector
myvector[2:3]
```

***

```{r}
sum(myvector)
```

*Vectorised operations* operate on a vector all at once:

```{r}
myvector * 2
```


Things you should know: data frames
========================================================

- The idea of a *data frame* (tightly coupled vectors)

```{r}
head(cars)  # one of R's built-in datasets

```

Data frames are the most frequently used data type in R.


Things you should know: control flow
========================================================

- Basic  *control flow* statements

```{r}
if(sum(1:4) == 10) {
  print("right!")
} else {
  print("wrong!")
}
```

```{r}
for(i in 1:4) { cat(i) }
```


Things you should know: scripts
========================================================

The difference between a *script* (stored program) and *command line* (immediate response).

In general, you want to use scripts, which provide *reproducibility*.


```{r, eval = FALSE}
source("myscript.R")
```

***

<img src="images/mayan_script.gif" />


Things you should know: packages
========================================================

- *Packages* are pieces of software that can be loaded into R. There are thousands, for all kinds of tasks and needs.

```{r, eval=FALSE}
# The single most popular R package is `ggplot2`
library(ggplot2)

# qplot:
# a "quick plot" function in ggplot2
qplot(speed, 
      dist, 
      data = cars)
```

***

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
qplot(speed, dist, data=cars)
```


Hands-on: examining the `iris` dataset
========================================================
type: prompt
incremental: false

Hands-on work in RStudio.
* Built-in datasets
* Using `summary`, `names`, `head`, `tail`
* Looking at particular rows and columns
* Subsetting the data
* Basic plots of the data


Computing on columns
========================================================

This can be simple...

```{r}
d <- data.frame(x = 1:3)
d$y <- d$x * 2
d$z <- cumsum(d$y) # cumulative sum
d$four <- ifelse(d$y == 4, "four", "not 4") 
d
```


Computing on columns
========================================================

...or more complex. For example, the [Picarro](http://www.picarro.com/products_solutions/trace_gas_analyzers/co_co2_ch4_h2o) gas analyzer produces data with multiplexer valve numbers (i.e., in an experiment, the multiplexer automatically switches between valves). Whenever the valve number changes, we want to assign a new sample number.

```{r}
# Toy data set
# Analyzer is switching between valves 1, 2, and 3
vn <- c(1, 1, 2, 3, 3, 3, 1, 2, 2, 3)
```

There are 6 samples here, and we want to produce `1, 1, 2, 3, 3, 3, 4, 5, 5, 6`.


Exercise: Computing on columns
========================================================

```{r}
# Works, but slow
samplenums <- rep(NA, length(vn))
samplenums[1] <- 1
s <- 1

# Loop and compare valve numbes one by one
for(i in 2:length(vn)) {
  if(vn[i] != vn[i-1])
    s <- s + 1
  samplenums[i] <- s
}
samplenums
```


Exercise: Computing on columns
========================================================

```{r}
# Vectorised: fast and elegant
newvalve <- vn[-length(vn)] != vn[-1]
newvalve <- c(TRUE, newvalve)
newvalve
cumsum(newvalve)  # "cumulative sum"
```


Exercise: Computing on columns - time
========================================================

```{r, echo=FALSE}
vnums <- rep(c(1, 2, 2, 3, 3, 3, 1, 1, 2, 3, 3), 100000)
```

This has big consequences!

For a data frame with `r prettyNum(length(vnums), big.mark=",")` rows:

**The larger lesson to take away here** is that in R, `for` loops are rarely the fastest way to do something (although they may be the clearest).

When possible, take advantage of R's *vectorised operations*.


***

```{r, cache=TRUE, echo=FALSE}
slowtime <- system.time({
  samplenums <- rep(1, length(vnums))
  s <- 1
  for(i in seq_along(vnums)[-1]) {
    if(vnums[i] != vnums[i-1])
      s <- s + 1
    samplenums[i] <- s
  }
})

fasttime <- system.time(cumsum(c(TRUE, vnums[-length(vnums)] != vnums[-1])))

d <- data.frame(Algorithm=c("for loop", "cumsum"), Time_s=c(slowtime[3], fasttime[3]))
library(ggplot2)
theme_set(theme_bw())
ggplot(d, aes(Algorithm, Time_s)) + geom_bar(stat='identity') + ylab("Time (seconds)")
```


Understanding and dealing with NA
========================================================

One of R's strengths is that missing values are a first-class data type: `NA`.

```{r}
x <- c(1, 2, 3, NA)
# Which are NA?
is.na(x)
any(is.na(x))
```

***

```{r}
which(is.na(x))
x[!is.na(x)]
```


Understanding and dealing with NA
========================================================

Like `NaN` and `Inf`, generally `NA` 'poisons' operations, so NA values must be explicitly ignored and/or removed.

```{r}
x <- c(1, 2, 3, NA)
sum(x) # NA
sum(x, na.rm = TRUE)
```


Dealing with dates
========================================================

R has a `Date` class representing calendar dates, and an `as.Date` function for converting to Dates. The `lubridate` package is often useful (and easier) for these cases:

```{r}
library(lubridate)
x <- c("09-01-01", "09-01-02") # character!
ymd(x)   # there's also dmy, ymd_hms, etc.
```

Once data are in `Date` format, the time interval between them can be computed simply by subtraction. See `?difftime`


Merging datasets
========================================================

Often, as we clean and reshape data, we want to merge different datasets together. The built-in `merge` command does this well.

Let's say we have a data frame containing information on how pretty each of the `iris` species is:

```{r, echo=FALSE}
howpretty <- data.frame(Species = unique(iris$Species), 
                        pretty = c("ugly", "ok", "lovely"))
howpretty
```


Merging datasets
========================================================

`merge` looks for names in common between two data frames, and uses these to merge.

```{r, eval=FALSE}
merge(iris, howpretty)
```

```{r, echo=FALSE}
head(merge(iris, howpretty)[c(1, 2, 6)])
```

(Viewing only a few columns and rows.) The `dplyr` package has more varied, faster database-style join operations.


Summarizing and manipulating data
========================================================
type: section


History lesson
========================================================

<img src="images/history.png" width="850" />


Summarizing and manipulating data
========================================================

Thinking back to the typical data pipeline, we often want to summarize data by groups as an intermediate or final step. For example, for each subgroup we might want to:

* Compute mean, max, min, etc. (`n`->1)
* Compute rolling mean and other window functions (`n`->`n`)
* Fit models and extract their parameters, goodness of fit, etc.

Specific examples:

* `cars`: for each speed, what's the farthest distance traveled?
* `iris`: how many samples were taken from each species?
* `babynames`: what's the most common name over time?


Split-apply-combine
========================================================

These are generally known as *split-apply-combine* problems.

<img src="images/split_apply_combine.png" width="600" />

From https://github.com/ramnathv/rblocks/issues/8


aggregate
========================================================

Base R has an `aggregate` function. It's not particularly fast or flexible, and confusingly it has different forms (syntax).

It can however be useful for simple operations:

```{r}
# What's the farthest distance at each speed?
aggregate(dist ~ speed, 
          data = cars, FUN = max)
```


dplyr
========================================================

The newer `dplyr` package specializes in data frames, recognizing that most people use them most of the time.

`dplyr` also allows you to work with remote, out-of-memory databases, using exactly the same tools, because it abstracts away *how* your data is stored.

`dplyr` is **extremely fast**.


Operation pipelines in R
========================================================

`dplyr` *imports*, and its examples make heavy use of, the [magrittr](https://github.com/smbache/magrittr) package, which introduces a **pipeline** operator `%>%` to R.

Not everyone is a fan of piping, and there are situations where it's not appropriate; but we'll stick to `dplyr` convention and use it frequently.


Operation pipelines in R
========================================================

Standard R notation:

```{r, eval=FALSE}
x <- read_my_data(f)
y <- merge_data(clean_data(x), otherdata)
z <- summarize_data(y)
```

Notation using a `magrittr` pipeline:

```{r, eval=FALSE}
read_my_data(f) %>%
  clean_data %>%
  merge_data(otherdata) %>%
  summarize_data ->
  z
```


Verbs
========================================================

`dplyr` provides functions for each basic *verb* of data manipulation. These tend to have analogues in base R, but use a consistent, compact syntax, and are very high performance.

* `filter()` - subset rows; like `base::subset()`
* `arrange()` - reorder rows; like `order()`
* `select()` - select columns
* `mutate()` - add new columns
* `summarise()` - like `aggregate`


Grouping
========================================================

`dplyr` verbs become particularly powerful when used in conjunction with *groups* we define in the dataset. The `group_by` function converts an existing data frame into a grouped `tbl`.

```{r}
library(dplyr)
cars %>%
  group_by(speed)
```


Summarizing cars
========================================================

We previously did this using `aggregate`. Now, `dplyr`:

```{r}
cars %>% 
  group_by(speed) %>% 
  summarise(max(dist))
```


Summarizing iris
========================================================

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise(msl = mean(Sepal.Length))
```


Summarizing iris
========================================================

We can apply (multiple) functions across (multiple) columns.

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise_each(funs(mean, median, sd), 
                 Sepal.Length)
```


Introduction to `babynames`
========================================================

```{r}
library(babynames)
babynames
```


Summarizing babynames
========================================================

What does this calculate?

```{r}
babynames %>%
  group_by(year, sex) %>% 
  summarise(prop = max(prop), 
            name = name[which.max(prop)])
```


Summarizing babynames
========================================================

<img src="images/popular_babynames.png" width="800" />

https://en.wikipedia.org/wiki/Linda_(1946_song)


Why use dplyr?
========================================================

* Clean, concise, and consistent syntax.

* In general `dplyr` is ~10x faster than the older `plyr` package. (And `plyr` was ~10x faster than base R.)

* Same code can work with data frames or remote databases.


Hands-on: manipulating the `babynames` dataset
========================================================
type: prompt
incremental: false

Load the dataset using `library(babynames)`.

Read its help page. Look at its structure (rows, columns, summary).

Use `dplyr` to calculate the total number of names in the SSA database for each year. 

Calculate the 5th most popular name for girls in each year. Hint: `nth()`.


Processing Picarro data
========================================================
type: section


Picarro data
========================================================

The Picarro outputs text files with names like 

>CFADS2283-20150831-171845Z-DataLog_User.dat 

>(1.2 MB)

They're bulky and I will often store them gzip'd or zip'd (R will transparently decompress on reading).

>CFADS2283-20150831-171845Z-DataLog_User.dat.gz

>0.1 MB


Picarro data
========================================================

These files are straightfoward text files.

<img src="images/datafile.png" width="800" />


Picarro data
========================================================

Data in the Picarro output stream include:

* **`DATE`: yyyy-mm-dd format**
* **`TIME`: hh:mm:ss.sss**
* Time in other forms: `FRAC_DAYS_SINCE_JAN1`, `FRAC_HRS_SINCE_JAN1`, `JULIAN_DAYS`, `EPOCH_TIME`
* `ALARM_STATUS` and `INST_STATUS`
* `species`
* **`MPVPosition` and `solenoid_valves`**
* **Gas data: `CH4`, `CH4_dry`, `CO2`, `CO2_dry`, `H2O`, `h2o_reported`**


Picarro data cautions
========================================================
type: alert
incremental: true

**Fractional `MPVPosition` valve numbers**

The analyzer records fractional valve numbers when switching between multiplexer valves. You'll want to discard these.

**Date and time stamps**

Know what time the analyzer is set to. If local time, does your experiment cross a daylight savings transition? Does it cross into a new year (which would screw up e.g. `FRAC_HRS_SINCE_JAN1`)?

I recommend setting it to UTC and LEAVE IT THERE. Then it's easy to convert the `DATE` and `TIME` fields into a true R date field and adjust to local time zone if necessary.


Getting data into R
========================================================

The most common way to bring data into R is via `read.table()`:
```{r, eval=FALSE}
d <- read.table("mydata.csv", header = TRUE)
```

The `readr` package provides `read_table()`, which is faster and easier to use.
```{r, eval=FALSE}
library(readr)
d <- read_table("mydata.csv")
```


Hands-on: Picarro data
========================================================
type: prompt

Let's open the `picarro.R` file, which I started but didn't finish.

At this point, we have the tools to complete the job. Can you help?


Things we didn't talk about
========================================================

- reading data into R (not much)
- working with non-text data
- reshaping data
- writing data
- graphing data



Last thoughts
========================================================

>The best thing about R is that it was written by statisticians. The worst thing about R is that it was written by statisticians.
>
>-- Bow Cowgill

All the source code for this presentation is available at https://github.com/bpbond/R-data-picarro


Resources
========================================================
type: section


Resources
========================================================

* [CRAN](http://cran.r-project.org) - The Comprehensive R Archive Network.
* [GitHub](https://github.com/JGCRI) - The JGCRI organization page on GitHub.
* [RStudio](http://www.rstudio.com) - the integrated development environment for R. Makes many things hugely easier.
* [Advanced R](http://adv-r.had.co.nz) - the companion website for “Advanced R”, a book in Chapman & Hall’s R Series. Detailed, in depth look at many of the issues covered here.


Resources
========================================================

R has many contributed *packages* across a wide variety of scientific fields. Almost anything you want to do will have packages to support it.

[CRAN](http://cran.r-project.org) also provides "Task Views". For example:

***

- Bayesian
- Clinical Trials
- Differential Equations
- Finance
- Genetics
- HPC
- Meta-analysis
- Optimization
- [**Reproducible Research**](http://cran.r-project.org/web/views/ReproducibleResearch.html)
- Spatial Statistics
- Time Series
