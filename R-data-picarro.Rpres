R and (Picarro) data
========================================================
author: Ben Bond-Lamberty
date: January 2016

A workshop covering R basics; reproducibility; data exploration, summarizing and manipulation; and issues specific to Picarro data.


The next three hours of your life
========================================================

* Introduction: R basics and reproducible research (45 minutes; hands-on: installing the packages we'll need)
* Examining and cleaning data (45 minutes; hands-on: the `iris` dataset)
* Summarizing and manipulating data (45 minutes; hands-on: the `babynames` dataset)
* Bringing things together: working with Picarro data (45 minutes; hands-on: Picarro data)

Feedback: <a href="mailto:bondlamberty@pnnl">Email</a>  [Twitter](https://twitter.com/BenBondLamberty)


Focus of this workshop
========================================================

<img src="images/pipeline.png" width="1000" />

In a typical data pipeline:
- *Raw data* can come from many sources 
- *Processing* - cleaning, modifying, reshaping, QC
- *Summarizing* - merging with other data, groupwise summaries
- *Products* include output data, plots, statistical analyses


Is R the right tool for the job?
========================================================

>R has simple and obvious appeal. Through R, you can sift through complex data sets, manipulate data through sophisticated modeling functions, and create sleek graphics to represent the numbers, in just a few lines of code...R’s greatest asset is the vibrant ecosystem has developed around it: The R community is constantly adding new packages and features to its already rich function sets.
>
>-- [The 9 Best Languages For Crunching Data](http://www.fastcompany.com/3030716/the-9-best-languages-for-crunching-data)


Is R the right tool for the job?
========================================================

It's also very useful during _Talk Like A Pirate Day_.

<img src="images/talk-like-a-pirate.gif" />


Is R the right tool for the job?
========================================================

But it might not be. R has limitations and weaknesses:
- nontrivial learning curve; quirks; inconsistent syntax
- documentation patchy and terse
- package quality varies
- generally operates in-memory only

There are other tools that might be better for your specific need!
- Python, C++, Hadoop, CDO/NCL, bash, ...
- Excel in _extremely_ limited circumstances


Reproducibility
========================================================
type: section


Reproducibility
========================================================

We are in the era of 'big data', but even if you work with 'little data' you have to acquire some skills to deal with those data.

**Most fundamentally, your results have to be reproducible.**

>Your most important collaborator is your future self. It’s important to make a workflow that you can use time and time again, and even pass on to others in such a way that you don’t have to be there to walk them through it. [Source](http://berkeleysciencereview.com/reproducible-collaborative-data-science/)

Reproducibility means *scripts* or *programs* tied to *open source software*.


You can't reproduce
========================================================
...what doesn't exist.
- Gozilla ate my computer
+ backup
+ ideally *continuous*
- Godzilla ate my office
+ cloud

***

<img src="images/godzilla.jpg" width="400" />


You can't reproduce
========================================================

...what you've lost. What if you need access to a file as it existed 1, 10, or 100, or 1000 days ago?
- Incremental backups (minimum)
- Version control (better). A *repository* holds files and tracks changes

***

<img src="images/tardis.jpg" width="400" />


Version control
========================================================

**Git** and **GitHub** are the most popular (and free) version control tools for use with R, and many other languages:
- version control
- sharing code with collaborators
- issue tracking
- social coding

***

<img src="images/github.png" width="400" />


Reproducible research example
========================================================

A typical project/paper directory for me:
```
0-download.R
1-process_data.R
2-analyze_data.R
3-make_graphs.R
logs/
output/
rawdata/
```

This directory is generated by my [default script](https://github.com/bpbond/R_analysis_script). It is backed up both *locally* and *remotely*, and under *version control*.


Reproducible research example
========================================================

- Sequentially numbered R scripts (`0-download.R`, `1-process_data.R`, ...)
- Each script depends on the previous one
- Each has a discrete, logical function
- Each produces a *log file* including date/time, what R version was used, etc.
- This analytical chain starts with raw data
- ...and ends with figures and tables for ms
+ *or the ms itself!* This presentation, for example, was generated directly from an R file


Hands-on: setting up R and RStudio
========================================================
type: prompt
incremental: false

If you're doing the exercises and problems, you'll need these
packages:
- `dplyr` - fast, flexible tool for working with data frames
- `ggplot2` - popular package for visualizing data

We'll also use this data package:
- `babynames` - names provided to the SSA 1880-2013

Finally, we'll download a [repository](https://github.com/bpbond/R-data-picarro) (collection of code and data) for this workshop.

Let's do that, and get oriented in **RStudio**, now.


R basics
========================================================
type: section


Things you should know: basics
========================================================

This workshop assumes you understand a few basics of R:

- What R is
- How to start and quit it
- How to get help

```{r, eval=FALSE}
# Get help for a specific function
?summary

# Get help for an entire package
help(package = 'ggplot2')
```


Things you should know: basics
========================================================

- The idea of *objects*, *functions*, *assignment*, and *comments*

```{r}
x <- 10 + 2 # `x` is an object
sum(x) # `sum` is a function
```

- The idea of *data types*
```{r}
x <- 3.14     # numeric
y <- "hello"  # character
z <- TRUE     # logical
# We're going to avoid this next one
f <- factor(c("apple", "pear", "banana")) 
```


Things you should know: vectors
========================================================

- The *vector* data type

```{r}
myvector <- 1:5
myvector
```

```{r}
myvector * 2
```

***

```{r}
length(myvector)
sum(myvector)
```


Things you should know: data frames
========================================================

- The idea of a *data frame* (tightly coupled vectors)

```r
head(cars)  # a built-in dataset
```

```
speed dist
1     4    2
2     4   10
3     7    4
4     7   22
5     8   16
6     9   10
```
Data frames are the most frequently used data type in R.


Things you should know: control flow
========================================================

- Basic  *control flow* statements

```{r}
if(sum(1:4) == 10) {
  print("right!")
} else {
  print("wrong!")
}
```

```{r}
for(i in 1:4) { cat(i) }
```


Things you should know: scripts
========================================================

The difference between a *script* (stored program) and *command line* (immediate response).

In general, you want to use scripts, which provide *reproducibility*.


```{r, eval = FALSE}
source("myscript.R")
```

***

<img src="images/mayan_script.gif" />


Things you should know: packages
========================================================

- *Packages* are pieces of software that can be optionally loaded into R. There are thousands, written for all kinds of tasks and needs.

```{r, eval=FALSE}
# The single most popular package is `ggplot2`
library(ggplot2)

# qplot = quick plot
qplot(speed, 
      dist, 
      data = cars)
```

***

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
qplot(speed, dist, data=cars)
```


Hands-on: examining the `iris` dataset
========================================================
type: prompt
incremental: false

Hands-on work in RStudio.
* Built-in datasets
* Using `summary`, `names`, `head`, `tail`
* Looking at particular rows and columns
* Subsetting the data
* Basic plots of the data


Computing on columns
========================================================

This can be simple...

```{r}
d <- data.frame(x=1:3)
d$y <- d$x * 2
d$z <- cumsum(d$y)
d$four <- ifelse(d$y == 4, "four", "not four") 
d
```

Base R provides a set of high performance functions for many of these tasks: `cumsum`, `colMeans`, `colSums`, `rowMeans`, `rowSums`, etc.


Computing on columns
========================================================

...or more complex. For example, for Picarro data comes with multiplexer valve numbers (i.e., in an experiment, the multiplexer automatically switches between valves). Whenever the valve number changes, we want to assign a new sample number.

```{r}
# Toy data set
# Analyzer is switching between valves 1, 2, and 3
vn <- c(1, 1, 2, 3, 3, 3, 1, 2, 2, 3)
```

There are 6 samples here, and we want to produce `1, 1, 2, 3, 3, 3, 4, 5, 5, 6`.


Exercise: Computing on columns
========================================================

```{r}
# Works, but slow
samplenums <- rep(NA, length(vn))
samplenums[1] <- 1
s <- 1
for(i in 2:length(vn)) {
  if(vn[i] != vn[i-1])
    s <- s + 1
  samplenums[i] <- s
}
samplenums
```


Exercise: Computing on columns
========================================================

```{r}
# Vectorised: fast and elegant
newvalve <- c(TRUE, vn[-length(vn)] != vn[-1])
newvalve
cumsum(newvalve)
```


Exercise: Computing on columns - time
========================================================

```{r, echo=FALSE}
vnums <- rep(c(1, 2, 2, 3, 3, 3, 1, 1, 2, 3, 3), 100000)
```

This has big consequences!

For a data frame with `r prettyNum(length(vnums), big.mark=",")` rows:

**The larger lesson to take away here** is that in R, `for` loops are rarely the fastest way to do something (although they may be the clearest).

***

```{r, cache=TRUE, echo=FALSE}
slowtime <- system.time({
  samplenums <- rep(1, length(vnums))
  s <- 1
  for(i in seq_along(vnums)[-1]) {
    if(vnums[i] != vnums[i-1])
      s <- s + 1
    samplenums[i] <- s
  }
})

fasttime <- system.time(cumsum(c(TRUE, vnums[-length(vnums)] != vnums[-1])))
d <- data.frame(Algorithm=c("for loop", "cumsum"), Time_s=c(slowtime[3], fasttime[3]))
library(ggplot2)
theme_set(theme_bw())
ggplot(d, aes(Algorithm, Time_s)) + geom_bar(stat='identity') + ylab("Time (seconds)")
```


Understanding and dealing with NA
========================================================

One of R's strengths is that missing values are a first-class data type: `NA`.

```{r}
x <- c(1, 2, 3, NA)
# Which values are NA?
is.na(x)
any(is.na(x))
```

***

```{r, eval=FALSE}
# What does this return?
which(is.na(x))
```


Understanding and dealing with NA
========================================================

Like `NaN` and `Inf`, generally `NA` 'poisons' operations, so NA values must be explicitly ignored and/or removed.

```{r}
sum(x) # NA
sum(x, na.rm = TRUE)
```


Dealing with dates
========================================================

R has a `Date` class representing calendar dates, and an `as.Date` function for converting to Dates. The `lubridate` package is often useful (and easier) for these cases:

```{r}
library(lubridate)
x <- c("09-01-01", "09-01-02") # character!
ymd(x)   # there's also dmy, mdy, ymd_hms, etc.
```

Once data are in `Date` format, the time interval between them can be computed simply by subtraction. See also `?difftime`


Merging datasets
========================================================

Often, as we clean and reshape data, we want to merge different datasets together. The built-in `merge` command does this well.

Let's say we have a data frame containing information on how pretty each of the `iris` species is:

```{r, echo=FALSE}
howpretty <- data.frame(Species = unique(iris$Species), 
                        pretty = c("ugly", "ok", "lovely"))
howpretty
```


Merging datasets
========================================================

`merge` looks for names in common between two data frames, and uses these to merge.

```{r, eval=FALSE}
merge(iris, howpretty)
```

```{r, echo=FALSE}
head(merge(iris, howpretty)[c(1, 2, 6)])
```

(NB - viewing only a few columns and rows.) The `dplyr` package has more varied, faster database-style join operations.


Summarizing and manipulating data
========================================================
type: section


History lesson
========================================================

<img src="images/history.png" width="850" />


Summarizing and manipulating data
========================================================

Thinking back to the typical data pipeline, we often want to summarize data by groups as an intermediate or final step. For example, for each subgroup we might want to:

* Compute mean, max, min, etc. (`n`->1)
* Compute rolling mean and other window functions (`n`->`n`)
* Fit models and extract their parameters, goodness of fit, etc.

Specific examples:

* `cars`: for each speed, what's the farthest distance traveled?
* `iris`: how many samples were taken from each species?
* `babynames`: what's the most common name over time?


Split-apply-combine
========================================================

These are generally known as *split-apply-combine* problems.

<img src="images/split_apply_combine.png" width="600" />

From https://github.com/ramnathv/rblocks/issues/8


The apply family
========================================================

Base R has the *apply* family of functions. Unfortunately they have inconsistent and confusing syntax, middling performance, and functional quirks. R also has an `aggregate` function. It's not particularly fast or flexible, and confusingly has different forms. 

It can however be useful for simple operations:

```{r}
aggregate(dist ~ speed, data=cars, FUN = max)
```


dplyr
========================================================

The newer `dplyr` package specializes in data frames, recognizing that most people use them most of the time, and is **extremely fast**.

`dplyr` also allows you to work with remote, out-of-memory databases, using exactly the same tools, because dplyr will translate your R code into the appropriate SQL.

In other words, `dplyr` abstracts away *how* your data is stored.


Operation pipelines in R
========================================================

`dplyr` *imports*, and its examples make heavy use of, the [magrittr](https://github.com/smbache/magrittr) package, which introduces a **pipeline** operator `%>%` to R.

Not everyone is a fan of piping, and there are situations where it's not appropriate; but we'll stick to `dplyr` convention and use it frequently.


Operation pipelines in R
========================================================

Standard R notation:

```{r, eval=FALSE}
x <- read_my_data(f)
y <- merge_data(clean_data(x), otherdata)
z <- summarize_data(y)
```

Notation using a `magrittr` pipeline:

```{r, eval=FALSE}
read_my_data(f) %>%
  clean_data() %>%
  merge_data(otherdata) %>%
  summarize_data() ->
  z
```


Verbs
========================================================

`dplyr` provides functions for each basic *verb* of data manipulation. These tend to have analogues in base R, but use a consistent, compact syntax, and are very high performance.

* `filter()` - subset rows; like `base::subset()`
* `arrange()` - reorder rows; is a wrapper for `order()`
* `select()` - select columns
* `mutate()` - add new columns; like `base::transform()
* `summarise()` - like `aggregate` or `plyr::ddply`


Grouping
========================================================

`dplyr` verbs become particularly powerful when used in conjunction with *groups* we define in the dataset. The `group_by` function converts an existing data frame into a grouped `tbl`.


```{r}
library(dplyr)
library(babynames)
group_by(babynames, year, sex)
```


Summarizing iris
========================================================

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise(msl = mean(Sepal.Length))
```


Summarizing iris
========================================================

We can apply (multiple) functions across (multiple) columns.

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise_each(funs(mean, median, sd), 
                 Sepal.Length)
```


Summarizing babynames
========================================================

Note that each summary operation peels off one grouping layer. What does this calculate?

```{r}
babynames %>%
  group_by(year, sex) %>% 
  summarise(prop = max(prop), 
            name = name[which.max(prop)])
```


Summarizing babynames
========================================================

<img src="images/popular_babynames.png" width="800" />

https://en.wikipedia.org/wiki/Linda_(1946_song)


Summarizing babynames
========================================================

In general `dplyr` is ~10x faster than the older `plyr` package, which in turn is ~10x faster than base R.

Base R also tends to require many more lines of code.


Hands-on: manipulating the `babynames` dataset
========================================================
type: prompt
incremental: false

Load the dataset using `library(babynames)`. Read its help page.

How many rows and columns are there in the `babynames` dataset? What name is in row #12345? How many unique baby names are there?

How many 19th century rows are there?

Make a new data frame with a random 1% of the original rows. Hint: `sample()`.

Calculate the 5th most popular name for girls in each year. Hint: `nth()`.


Processing Picarro data
========================================================
type: section


Picarro data
========================================================

The Picarro outputs text files with names like 

>CFADS2283-20150831-171845Z-DataLog_User.dat 

>(1.2 MB)

They're bulky and I will often store them gzip'd or zip'd (R doesn't care and will transparently decompress them on reading).

>CFADS2283-20150831-171845Z-DataLog_User.dat.gz

>0.1 MB


Picarro data
========================================================

These files are straightfoward text files.

<img src="images/datafile.png" width="800" />


Picarro data
========================================================

Data in the Picarro output stream include:

* **`DATE`: yyyy-mm-dd format**
* **`TIME`: hh:mm:ss.sss**
* Time in other forms: `FRAC_DAYS_SINCE_JAN1`, `FRAC_HRS_SINCE_JAN1`, `JULIAN_DAYS`, `EPOCH_TIME`
* `ALARM_STATUS` and `INST_STATUS`
* `species`
* **`MPVPosition` and `solenoid_valves`**
* **Gas data: `CH4`, `CH4_dry`, `CO2`, `CO2_dry`, `H2O`, `h2o_reported`**


Picarro data cautions
========================================================
type: alert
incremental: true

**Fractional `MPVPosition` valve numbers**

The analyzer records fractional valve numbers when switching between multiplexer valves. You'll want to discard these.

**Date and time stamps**

Know what time the analyzer is set to. If local time, does your experiment cross a daylight savings transition? Does it cross into a new year (which would screw up e.g. `FRAC_HRS_SINCE_JAN1`)?

I recommend setting it to UTC and LEAVE IT THERE. Then it's easy to convert the `DATE` and `TIME` fields into a true R date field and adjust to local time zone if necessary.


Hands-on: Picarro data
========================================================
type: prompt

Let's open the `picarro.R` file, which I started but didn't finish.

At this point, we have the tools to complete the job. Can you help?


Picarro pipeline solution
========================================================

```{r, eval=FALSE}
rawdata %>%
  filter(MPVPosition == floor(MPVPosition)) %>%
  select(samplenum, DATETIME, MPVPosition, 
         CH4_dry, CO2_dry) %>%
  left_join(valvedata, by = "MPVPosition") %>%
  group_by(samplenum) %>%
  mutate(secs = difftime(DATETIME, 
                         min(DATETIME), 
                         units = "secs"),
         secs = as.numeric(secs)) %>%
  filter(soilcore != "J") ->
  cleandata
```


Last thoughts
========================================================

>The best thing about R is that it was written by statisticians. The worst thing about R is that it was written by statisticians.
>
>-- Bow Cowgill

All the source code for this presentation is available at https://github.com/bpbond/R-data-picarro


Resources
========================================================
type: section


Resources
========================================================

* [CRAN](http://cran.r-project.org) - The Comprehensive R Archive Network. Ground zero for R.
* [GitHub](https://github.com/JGCRI) - The JGCRI organization page on GitHub.
* [RStudio](http://www.rstudio.com) - the integrated development environment for R. Makes many things hugely easier.
* [Advanced R](http://adv-r.had.co.nz) - the companion website for “Advanced R”, a book in Chapman & Hall’s R Series. Detailed, in depth look at many of the issues covered here.


Resources
========================================================

R has many contributed *packages* across a wide variety of scientific fields. Almost anything you want to do will have packages to support it.

[CRAN](http://cran.r-project.org) also provides "Task Views". For example:

***

- Bayesian
- Clinical Trials
- Differential Equations
- Finance
- Genetics
- HPC
- Meta-analysis
- Optimization
- [**Reproducible Research**](http://cran.r-project.org/web/views/ReproducibleResearch.html)
- Spatial Statistics
- Time Series
