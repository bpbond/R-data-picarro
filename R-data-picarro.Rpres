R and (Picarro) data
========================================================
author: Ben Bond-Lamberty
date: January 2016

A workshop covering R basics; reproducibility; data exploration, summarizing and manipulation; and issues specific to Picarro data.


The next three hours of your life
========================================================

* Introduction: R basics and reproducible research (45 minutes; hands-on: installing the packages we'll need)
* Examining and cleaning data (45 minutes; hands-on: the `iris` dataset)
* Summarizing and manipulating data (45 minutes; hands-on: the `babynames` dataset)
* Bringing things together: working with Picarro data (45 minutes; hands-on: Picarro data)

Three hours, 25% lecture and 75% working examples and problems.

Feedback: <a href="mailto:bondlamberty@pnnl">Email</a>  [Twitter](https://twitter.com/BenBondLamberty)


Focus of this workshop
========================================================

<img src="images/pipeline.png" width="1000" />

In a typical data pipeline:
- *Raw data* can come from many sources 
- *Processing* - cleaning, modifying, reshaping, QC
- *Summarizing* - merging with other data, groupwise summaries
- *Products* include output data, plots, statistical analyses


Is R the right tool for the job?
========================================================

>R has simple and obvious appeal. Through R, you can sift through complex data sets, manipulate data through sophisticated modeling functions, and create sleek graphics to represent the numbers, in just a few lines of code...R’s greatest asset is the vibrant ecosystem has developed around it: The R community is constantly adding new packages and features to its already rich function sets.
>
>-- [The 9 Best Languages For Crunching Data](http://www.fastcompany.com/3030716/the-9-best-languages-for-crunching-data)


Is R the right tool for the job?
========================================================

It's also very useful during _Talk Like A Pirate Day_.

<img src="images/talk-like-a-pirate.gif" />


Is R the right tool for the job?
========================================================

But it might not be. R has limitations and weaknesses:
- nontrivial learning curve; quirks; inconsistent syntax
- documentation patchy and terse
- package quality varies
- generally operates in-memory only

There are other tools that might be better for your specific need!
- Python, C++, Hadoop, CDO/NCL, bash, ...
- Excel in _extremely_ limited circumstances


Reproducibility
========================================================
type: section


Reproducibility
========================================================

We are in the era of 'big data', but even if you work with 'little data' you have to acquire some skills to deal with those data.

**Most fundamentally, your results have to be reproducible.**

**Reproducible by yourself, and others.**

**At any time in the future.**


You can't reproduce
========================================================
...what doesn't exist.
- Gozilla ate my computer
+ backup
+ ideally *continuous*
- Godzilla ate my office
+ cloud

***

<img src="images/godzilla.jpg" width="400" />


You can't reproduce
========================================================

...what you've lost. What if you need access to a file as it existed 1, 10, or 100, or 1000 days ago?
- Incremental backups (minimum)
- Version control (better). A *repository* holds files and tracks changes

***

<img src="images/tardis.jpg" width="400" />


Version control
========================================================

**Git** and **GitHub** are the most popular (and free) version control tools for use with R, and many other languages:
- version control
- sharing code with collaborators
- issue tracking
- social coding

***

<img src="images/github.png" width="400" />


Reproducible research example
========================================================

A typical project/paper directory for me:
```
0-download.R
1-process_data.R
2-analyze_data.R
3-make_graphs.R
logs/
output/
rawdata/
```

This directory is generated by my [default script](https://github.com/bpbond/R_analysis_script). It is backed up both *locally* and *remotely*, and under *version control*.


Reproducible research example
========================================================

- Sequentially numbered R scripts (`0-download.R`, `1-process_data.R`, ...)
- Each script depends on the previous one
- Each has a discrete, logical function
- Each produces a *log file* including date/time, what R version was used, etc.
- This analytical chain starts with raw data
- ...and ends with figures and tables for ms
+ *or the ms itself!* This presentation, for example, was generated directly from an R file


Hands-on: setting up R and RStudio
========================================================
type: prompt
incremental: false

If you're doing the exercises and problems, you'll need these
packages:
- `dplyr` - fast, flexible tool for working with data frames
- `ggplot2` - popular package for visualizing data

We'll also use this data package:
- `babynames` - names provided to the SSA 1880-2013

Finally, we'll download a [repository](https://github.com/bpbond/R-data-picarro) (collection of code and data) for this workshop.

Let's do that, and get oriented in **RStudio**, now.


R basics
========================================================
type: section


Things you should know: basics
========================================================

This workshop assumes you understand the basics of using R:

- What R is
- How to start and quit it
- How to get help

```{r, eval=FALSE}
# Get help for a specific function
?summary

# Get help for an entire package
help(package = 'ggplot2')
```


Things you should know: basics
========================================================

- The idea of *objects*, *functions*, *assignment*, and *comments*

```{r}
x <- 10 + 2 # `x` is an object
sum(x) # `sum` is a function
```

- The idea of *data types*
```{r}
x <- 3.14     # numeric
y <- "hello"  # character
z <- TRUE     # logical
# We're going to avoid this next one
f <- factor(c("apple", "pear", "banana")) 
```


Things you should know: vectors
========================================================

- The *vector* data type

```{r}
myvector <- 1:5
myvector
```

```{r}
myvector * 2
```

***

```{r}
length(myvector)
sum(myvector)
```


Things you should know: data frames
========================================================

- The idea of a *data frame* (tightly coupled vectors)

```r
head(cars)  # a built-in dataset
```

```
  speed dist
1     4    2
2     4   10
3     7    4
4     7   22
5     8   16
6     9   10
```
Data frames are the fundamental (in the sense of most frequently used) data type in R.


Things you should know: control flow
========================================================

- Most common control flow uses `if...then...else` and `for`

```{r}
if(sum(1:4) == 10) {
  print("right!")
} else {
  print("wrong!")
}
```

```{r}
for(i in 1:4) { cat(i) }
```


Things you should know: scripts
========================================================

The difference between a *script* (stored program) and *command line* (immediate response).

In general, you want to use scripts, which provide *reproducibility*.


```{r, eval = FALSE}
source("myscript.R")
```

***

<img src="images/mayan_script.gif" />


Things you should know: packages
========================================================

- *Packages* are pieces of software that can be optionally loaded and used. The package system is one of R's enormous strengths: there are thousands, written for all kinds of tasks and needs.

```{r, eval=FALSE}
library(ggplot2)
qplot(speed, dist, data = cars)
```

***

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
qplot(speed, dist, data=cars)
```


Hands-on: examining the `iris` dataset
========================================================
type: prompt
incremental: false

Hands-on work in RStudio.
* Built-in datasets
* Using `summary`, `names`, `head`, `tail`
* Looking at particular rows and columns
* Subsetting the data
* Basic plots of the data


Exercise: Examining data frames
========================================================
type: prompt


```r
library(babynames)
summary(babynames)
```

How many rows and columns are there in the `babynames` dataset?

What name is in row #12345?

How many unique baby names are there?

Make a new data frame with a random 1% of the original rows.

How many 19th century rows are there?


Exercise: Examining data frames
========================================================
type: prompt
incremental: true


```r
cat(dim(babynames), 
    as.character(babynames[12345, "name"]), 
    length(unique(babynames$name)))
```

```
1792091 5 Baxter 92600
```

```r
s <- babynames[sample(1:nrow(babynames), 
                      0.01 * nrow(babynames)),]

sum(babynames$year < 1900) # faster and more memory-efficient than nrow(subset(...))
```

```
[1] 52265
```


Cleaning data
========================================================

Usually, the first thing you'd like to do after importing data is *clean* it.

- change column types
- computing on columns
- splitting columns
- combining columns
- dealing with `NA` values


Computing on columns
========================================================

This can be simple...

```{r}
d <- data.frame(x=1:3)
d$y <- d$x * 2
d$z <- cumsum(d$y)
d$four <- ifelse(d$y == 4, "four", "not four") 
d
```

R provides a set of high performance functions for many of these tasks: `cumsum`, `colMeans`, `colSums`, `rowMeans`, `rowSums`, etc.


Exercise: Computing on columns
========================================================
type: prompt
incremental: true

One recent problem I had involved a data frame with multiplexer valve numbers; in the experiment, the multiplexer was automatically switching between valves.

Whenever the valve number changes, we want to assign a new sample number.

```{r}
# analyzer is switching between valves 1, 2, and 3
vnums <- c(1, 1, 2, 3, 3, 3, 1, 2, 2, 3)
# there are 6 samples here: (1, 1), (2), (3, 3, 3), (1), (2, 2), (3)

# There are ≥ two solutions
```


Exercise: Computing on columns
========================================================
type: prompt
incremental: true


```{r}
# Works, but slow
samplenums <- rep(1, length(vnums))
s <- 1
for(i in seq_along(vnums)[-1]) {
  if(vnums[i] != vnums[i-1])
    s <- s + 1
  samplenums[i] <- s
}
samplenums
```

```{r}
# Vectorised: fast and elegant
newvalve <- c(TRUE,
              vnums[-length(vnums)] != vnums[-1])
cumsum(newvalve)
```


Combining columns
========================================================

Combining columns is generally easy.

```r
d <- data.frame(x=1:3, y=4:6)
d$z <- with(d, paste("pasted", x, "and", y))  # note use of `with` here
d
```

```
  x y              z
1 1 4 pasted 1 and 4
2 2 5 pasted 2 and 5
3 3 6 pasted 3 and 6
```


Understanding and dealing with NA
========================================================

One of R's real strengths is that missing values are a first-class data type: `NA`.

```{r}
x <- c(1, 2, 3, NA)
is.na(x) # returns c(F, F, F, T)
any(is.na(x)) # returns TRUE
which(is.na(x)) # returns ...?
```

Like `NaN` and `Inf`, generally `NA` 'poisons' operations, so it must be handled.

```{r}
sum(x) # NA
sum(x, na.rm=TRUE) # 6
na.omit(data.frame(x))  # remove rows with NA
```


Dealing with dates
========================================================

R has a `Date` class representing calendar dates, and an `as.Date` function for converting to Dates. The `lubridate` package is often useful (and easier) for these cases:

```{r}
library(lubridate)
x <- c("09-01-01", "09-01-02")
ymd(x)   # there's also dmy and mdy!
```

Once data are in `Date` format, the time interval between them can be computed simply by subtraction. See also `?difftime`


Quiz: Cleaning Data
========================================================
type: prompt
incremental: true


```r
x <- -2:2
y <- 4/x 
y  # prints...?
```

```
[1]  -2  -4 Inf   4   2
```

```r
y <- y[is.finite(y)]
y  # prints...?
```

```
[1] -2 -4  4  2
```

***


```r
is.numeric(NA)
```

```
[1] FALSE
```

```r
is.numeric(Inf)
```

```
[1] TRUE
```

```r
is.infinite(Inf)
```

```
[1] TRUE
```


Merging datasets
========================================================

Often, as we clean and reshape data, we want to merge different datasets together. The built-in `merge` command does this well.

Let's say we have a data frame containing information on how pretty each of the `iris` species is:

```{r, echo=FALSE}
howpretty <- data.frame(Species = unique(iris$Species), 
                        pretty = c("ugly", "ok", "lovely"))
howpretty
```


Merging datasets
========================================================

`merge` looks for names in common between two data frames, and uses these to merge. Options allow us to control the merge behavior.

```{r, eval=FALSE}
merge(iris, howpretty)
```

```{r, echo=FALSE}
head(merge(iris, howpretty)[c(1, 2, 6)])
```

The `dplyr` package has more varied, faster database-style join operations.


Summarizing and operating on data
========================================================
type: section


History lesson
========================================================

<img src="images/history.png" width="850" />


Summarizing and operating on data
========================================================

Thinking back to the typical data pipeline, we often want to summarize data by groups as an intermediate or final step. For example, for each subgroup we might want to:

* Compute mean, max, min, etc. (`n`->1)
* Compute rolling mean and other window functions (`n`->`n`)
* Fit models and extract their parameters, goodness of fit, etc.

Specific examples:

* `cars`: for each speed, what's the farthest distance traveled?
* `iris`: how many samples were taken from each species?
* `babynames`: what's the most common name over time?


Split-apply-combine
========================================================

These are generally known as *split-apply-combine* problems.

<img src="images/splitapply.png" width="650" />

From https://ramnathv.github.io/pycon2014-r/explore/sac.html


The apply family
========================================================

Traditionally the *apply* family of functions was R's solution. Unfortunately they have inconsistent and confusing syntax, middling performance, and functional quirks.

Function      | Description
------------- | ------------
base::apply   |  Apply Functions Over Array Margins
base::by      |  Apply a Function to a Data Frame Split by Factors
base::eapply  |  Apply a Function Over Values in an Environment
**base::lapply**  |  **Apply a Function over a List or Vector**
base::mapply  |  Apply a Function to Multiple List or Vector Arguments
base::rapply  |  Recursively Apply a Function to a List
base::tapply  |  Apply a Function Over a Ragged Array

https://nsaunders.wordpress.com/2010/08/20/a-brief-introduction-to-apply-in-r/ provides a simple, readable summary of these.


aggregate
========================================================

R also has a built-in `aggregate` function. It's not particularly fast or flexible, and confusingly has a number of different forms. 

It can however be useful for simple operations:

```{r}
aggregate(dist ~ speed, data=cars, FUN=max)
```


dplyr
========================================================

The newer `dplyr` package makes a different tradeoff: it specializes in data frames, recognizing that most people use them most of the time, and is extremely fast.

`dplyr` also allows you to work with remote, out-of-memory databases, using exactly the same tools, because dplyr will translate your R code into the appropriate SQL.

In other words, `dplyr` abstracts away *how* your data is stored.


Operation pipelines in R
========================================================

`dplyr` *imports*, and its examples make heavy use of, the [magrittr](https://github.com/smbache/magrittr) package, which changes R syntax (remember, every operation is a function) to introduce a **pipe** operator `%>%`. This 
* structures sequences of data operations left-to-right (as opposed to inside-out)
* avoids nested function calls
* minimizes the need for local variables
* makes it easy to add steps anywhere in a sequence of operations

Not everyone is a fan of piping, and there are situations where it's not appropriate; but we'll stick to `dplyr` convention and use it frequently.


Operation pipelines in R
========================================================

Standard R notation:

```r
x <- read_my_data(f)
y <- process_data(clean_data(x), otherdata)
z <- summarize_data(y)
```

Notation using a `magrittr` pipe:

```r
z <- read_my_data(f) %>%
  clean_data() %>%
  process_data(otherdata) %>%
  summarize_data()
```


Operation pipelines in R
========================================================

Basic `magrittr` piping summary:

* `x %>% f` is equivalent to `f(x)`
* `x %>% f(y)` is equivalent to `f(x, y)`
* `x %>% f %>% g %>% h` is equivalent to `h(g(f(x)))`
* `x %>% f(y, .)` is equivalent to `f(y, x)`

Remember, `magrittr` is independent of `dplyr` - you can use pipes anywhere useful.


Verbs
========================================================

`dplyr` provides functions for each basic *verb* of data manipulation. These tend to have analogues in base R, but use a consistent, compact syntax, and are very high performance.

* `filter()` - subset rows; like `base::subset()`
* `arrange()` - reorder rows; is a wrapper for `order()`
* `select()` - select columns
* `mutate()` - add new columns; like `base::transform()
* `summarise()` - like `aggregate` or `plyr::ddply`


Grouping
========================================================

`dplyr` verbs become particularly powerful when used in conjunction with *groups* we define in the dataset. The `group_by` function converts an existing data frame into a grouped `tbl`.


```{r}
group_by(babynames, year, sex)
```


Summarizing iris
========================================================

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise(msl = mean(Sepal.Length))
```


Summarizing iris
========================================================

We can apply (multiple) functions across (multiple) columns.

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise_each(funs(mean, sd), 
                 Petal.Width, Sepal.Length) %>%
  names()
```


Summarizing babynames
========================================================

Note that each summary operation peels off one grouping layer. What does this calculate?

```r
babynames %>%
  group_by(year, sex) %>% 
  summarise(prop = max(prop), 
            name = name[which.max(prop)])
```

```
Source: local data frame [268 x 4]
Groups: year

   year sex       prop name
1  1880   F 0.07238359 Mary
2  1880   M 0.08154561 John
3  1881   F 0.06998999 Mary
4  1881   M 0.08098075 John
5  1882   F 0.07042473 Mary
6  1882   M 0.07831552 John
7  1883   F 0.06673052 Mary
8  1883   M 0.07907113 John
9  1884   F 0.06698985 Mary
10 1884   M 0.07648564 John
..  ... ...        ...  ...
```


Summarizing babynames
========================================================



<img src="images/babynames.png" width="850" />

https://en.wikipedia.org/wiki/Linda_(1946_song)


Summarizing babynames
========================================================

Time to summarize the 1.8 million row `babynames` using base R, `plyr`, and `dplyr`.

In general `dplyr` is ~10x faster than `plyr`, which in turn is ~10x faster than base R.

Base R also tends to require many more lines of code.

***

![plot of chunk unnamed-chunk-74](R-data-workshop-figure/unnamed-chunk-74-1.png) 


Useful summary functions
========================================================

`dplyr` provides some useful summarizing functions in addition to those provided by base R:

* `n()` - number of observations in current group
* `n_distinct(x)` - number of unique values in `x`
* `first(x)`, `last(x)`, `nth(x, n)` - extract particular elements from a group. These all have an `order_by` parameter to specify an ordering


Window functions
========================================================

`dplyr` *window functions* take `n` values and return `n` values. This is useful for computing lags, ranks, etc. For example, the popularity of "Mary":


```r
babynames %>% 
  group_by(year, sex) %>% 
  mutate(rank = dense_rank(desc(prop))) %>%
  filter(name == "Mary")
```



<img src="images/babynames_rank.png" width="950" />


Exercise: Summarizing data
========================================================
type: prompt

1. Use `dplyr` and the `babynames` data to calculate the 5th most popular name for girls in each year.


Exercise: Summarizing data
========================================================
type: prompt

```{r}
babynames %>% 
  filter(sex == 'F') %>% 
  group_by(year) %>% 
  summarise(fifth = nth(name, 5, 
                      order_by = desc(prop)))
```




Last thoughts
========================================================

>The best thing about R is that it was written by statisticians. The worst thing about R is that it was written by statisticians.
>
>-- Bow Cowgill

All the source code for this presentation is available at https://github.com/bpbond/R-data-workshop


Resources
========================================================
type: section


Resources
========================================================

* [CRAN](http://cran.r-project.org) - The Comprehensive R Archive Network. Ground zero for R.
* [GitHub](https://github.com/JGCRI) - The JGCRI organization page on GitHub.
* [RStudio](http://www.rstudio.com) - the integrated development environment for R. Makes many things hugely easier.
* [Advanced R](http://adv-r.had.co.nz) - the companion website for “Advanced R”, a book in Chapman & Hall’s R Series. Detailed, in depth look at many of the issues covered here.


Resources
========================================================

R has many contributed *packages* across a wide variety of scientific fields. Almost anything you want to do will have packages to support it.

[CRAN](http://cran.r-project.org) also provides "Task Views". For example:

***

- Bayesian
- Clinical Trials
- Differential Equations
- Finance
- Genetics
- HPC
- Meta-analysis
- Optimization
- [**Reproducible Research**](http://cran.r-project.org/web/views/ReproducibleResearch.html)
- Spatial Statistics
- Time Series


Removed slides
========================================================
type: section


Things you should know: data types
========================================================

- The *factor* data type

```r
# 'letters' and 'LETTERS' are built-in
summary(letters)
```

```
   Length     Class      Mode 
       26 character character 
```

```r
summary(as.factor(letters))
```

```
a b c d e f g h i j k l m n o p q r s t u v w x y z 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
```


Reshaping the Pew data
========================================================

Can have more than one `id` variable in your rows or columns.


```r
dcast(pew_long, religion + variable ~ ., mean)
```

```
                   religion           variable    .
1                  Agnostic              <$10k   27
2                  Agnostic            $10-20k   34
3                  Agnostic            $20-30k   60
4                  Agnostic            $30-40k   81
5                  Agnostic            $40-50k   76
6                  Agnostic            $50-75k  137
7                  Agnostic           $75-100k  122
8                  Agnostic          $100-150k  109
9                  Agnostic              >150k   84
10                 Agnostic Don't know/refused   96
11                  Atheist              <$10k   12
12                  Atheist            $10-20k   27
13                  Atheist            $20-30k   37
14                  Atheist            $30-40k   52
15                  Atheist            $40-50k   35
16                  Atheist            $50-75k   70
17                  Atheist           $75-100k   73
18                  Atheist          $100-150k   59
19                  Atheist              >150k   74
20                  Atheist Don't know/refused   76
21                 Buddhist              <$10k   27
22                 Buddhist            $10-20k   21
23                 Buddhist            $20-30k   30
24                 Buddhist            $30-40k   34
25                 Buddhist            $40-50k   33
26                 Buddhist            $50-75k   58
27                 Buddhist           $75-100k   62
28                 Buddhist          $100-150k   39
29                 Buddhist              >150k   53
30                 Buddhist Don't know/refused   54
31                 Catholic              <$10k  418
32                 Catholic            $10-20k  617
33                 Catholic            $20-30k  732
34                 Catholic            $30-40k  670
35                 Catholic            $40-50k  638
36                 Catholic            $50-75k 1116
37                 Catholic           $75-100k  949
38                 Catholic          $100-150k  792
39                 Catholic              >150k  633
40                 Catholic Don't know/refused 1489
41       Don’t know/refused              <$10k   15
42       Don’t know/refused            $10-20k   14
43       Don’t know/refused            $20-30k   15
44       Don’t know/refused            $30-40k   11
45       Don’t know/refused            $40-50k   10
46       Don’t know/refused            $50-75k   35
47       Don’t know/refused           $75-100k   21
48       Don’t know/refused          $100-150k   17
49       Don’t know/refused              >150k   18
50       Don’t know/refused Don't know/refused  116
51         Evangelical Prot              <$10k  575
52         Evangelical Prot            $10-20k  869
53         Evangelical Prot            $20-30k 1064
54         Evangelical Prot            $30-40k  982
55         Evangelical Prot            $40-50k  881
56         Evangelical Prot            $50-75k 1486
57         Evangelical Prot           $75-100k  949
58         Evangelical Prot          $100-150k  723
59         Evangelical Prot              >150k  414
60         Evangelical Prot Don't know/refused 1529
61                    Hindu              <$10k    1
62                    Hindu            $10-20k    9
63                    Hindu            $20-30k    7
64                    Hindu            $30-40k    9
65                    Hindu            $40-50k   11
66                    Hindu            $50-75k   34
67                    Hindu           $75-100k   47
68                    Hindu          $100-150k   48
69                    Hindu              >150k   54
70                    Hindu Don't know/refused   37
71  Historically Black Prot              <$10k  228
72  Historically Black Prot            $10-20k  244
73  Historically Black Prot            $20-30k  236
74  Historically Black Prot            $30-40k  238
75  Historically Black Prot            $40-50k  197
76  Historically Black Prot            $50-75k  223
77  Historically Black Prot           $75-100k  131
78  Historically Black Prot          $100-150k   81
79  Historically Black Prot              >150k   78
80  Historically Black Prot Don't know/refused  339
81        Jehovah's Witness              <$10k   20
82        Jehovah's Witness            $10-20k   27
83        Jehovah's Witness            $20-30k   24
84        Jehovah's Witness            $30-40k   24
85        Jehovah's Witness            $40-50k   21
86        Jehovah's Witness            $50-75k   30
87        Jehovah's Witness           $75-100k   15
88        Jehovah's Witness          $100-150k   11
89        Jehovah's Witness              >150k    6
90        Jehovah's Witness Don't know/refused   37
91                   Jewish              <$10k   19
92                   Jewish            $10-20k   19
93                   Jewish            $20-30k   25
94                   Jewish            $30-40k   25
95                   Jewish            $40-50k   30
96                   Jewish            $50-75k   95
97                   Jewish           $75-100k   69
98                   Jewish          $100-150k   87
99                   Jewish              >150k  151
100                  Jewish Don't know/refused  162
101           Mainline Prot              <$10k  289
102           Mainline Prot            $10-20k  495
103           Mainline Prot            $20-30k  619
104           Mainline Prot            $30-40k  655
105           Mainline Prot            $40-50k  651
106           Mainline Prot            $50-75k 1107
107           Mainline Prot           $75-100k  939
108           Mainline Prot          $100-150k  753
109           Mainline Prot              >150k  634
110           Mainline Prot Don't know/refused 1328
111                  Mormon              <$10k   29
112                  Mormon            $10-20k   40
113                  Mormon            $20-30k   48
114                  Mormon            $30-40k   51
115                  Mormon            $40-50k   56
116                  Mormon            $50-75k  112
117                  Mormon           $75-100k   85
118                  Mormon          $100-150k   49
119                  Mormon              >150k   42
120                  Mormon Don't know/refused   69
121                  Muslim              <$10k    6
122                  Muslim            $10-20k    7
123                  Muslim            $20-30k    9
124                  Muslim            $30-40k   10
125                  Muslim            $40-50k    9
126                  Muslim            $50-75k   23
127                  Muslim           $75-100k   16
128                  Muslim          $100-150k    8
129                  Muslim              >150k    6
130                  Muslim Don't know/refused   22
131                Orthodox              <$10k   13
132                Orthodox            $10-20k   17
133                Orthodox            $20-30k   23
134                Orthodox            $30-40k   32
135                Orthodox            $40-50k   32
136                Orthodox            $50-75k   47
137                Orthodox           $75-100k   38
138                Orthodox          $100-150k   42
139                Orthodox              >150k   46
140                Orthodox Don't know/refused   73
141         Other Christian              <$10k    9
142         Other Christian            $10-20k    7
143         Other Christian            $20-30k   11
144         Other Christian            $30-40k   13
145         Other Christian            $40-50k   13
146         Other Christian            $50-75k   14
147         Other Christian           $75-100k   18
148         Other Christian          $100-150k   14
149         Other Christian              >150k   12
150         Other Christian Don't know/refused   18
151            Other Faiths              <$10k   20
152            Other Faiths            $10-20k   33
153            Other Faiths            $20-30k   40
154            Other Faiths            $30-40k   46
155            Other Faiths            $40-50k   49
156            Other Faiths            $50-75k   63
157            Other Faiths           $75-100k   46
158            Other Faiths          $100-150k   40
159            Other Faiths              >150k   41
160            Other Faiths Don't know/refused   71
161   Other World Religions              <$10k    5
162   Other World Religions            $10-20k    2
163   Other World Religions            $20-30k    3
164   Other World Religions            $30-40k    4
165   Other World Religions            $40-50k    2
166   Other World Religions            $50-75k    7
167   Other World Religions           $75-100k    3
168   Other World Religions          $100-150k    4
169   Other World Religions              >150k    4
170   Other World Religions Don't know/refused    8
171            Unaffiliated              <$10k  217
172            Unaffiliated            $10-20k  299
173            Unaffiliated            $20-30k  374
174            Unaffiliated            $30-40k  365
175            Unaffiliated            $40-50k  341
176            Unaffiliated            $50-75k  528
177            Unaffiliated           $75-100k  407
178            Unaffiliated          $100-150k  321
179            Unaffiliated              >150k  258
180            Unaffiliated Don't know/refused  597
```
