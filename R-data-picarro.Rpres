(More) reproducible data analysis using R
========================================================
author: Ben Bond-Lamberty
date: August 2016
font-family: 'Helvetica'

A workshop covering reproducibility and repository design; data cleaning and reshaping using `tidyr`; and using the `dplyr` package for computation.

JGCRI


The plan
========================================================

* Introduction: reproducible research and repository design (45 minutes; hands-on: installing the packages we'll need)
* Examining and cleaning data (45 minutes; hands-on: the `iris` and `Pew` datasets, CMIP5 example)
* Summarizing and manipulating data (90 minutes; hands-on: the `babynames` dataset)

**This workshop assumes you understand the basics of R.**

Feedback: <a href="mailto:bondlamberty@pnnl">bondlamberty@pnnl.gov</a> or  [@BenBondLamberty](https://twitter.com/BenBondLamberty).


Reproducibility
========================================================
type: section


Reproducibility
========================================================

We are in the era of collaborative 'big data', but even if you work by yourself with 'little data' you have to have some skills to deal with those data, now and in the future.

>Your most important collaborator is your future self. It’s important to make a workflow that you can use time and time again, and even pass on to others in such a way that you don’t have to be there to walk them through it. [Source](http://berkeleysciencereview.com/reproducible-collaborative-data-science/)

Reproducibility generally means *scripts* or *programs* tied to *open source software*.


Reproducibility
========================================================

...is not all or nothing. A great, succinct resource here is Karl Broman's "initial steps toward reproducible research" webpage:

http://kbroman.org/steps2rr/

>Organizing analyses so that they are reproducible is not easy. It requires diligence and a considerable investment of time: to learn new computational tools, and to organize and document analyses as you go.

>But partially reproducible is better than not at all reproducible. Just try to make your next paper or project better organized than the last.


You can't reproduce
========================================================
...what doesn't exist.

**Gozilla ate my computer!**
* *automated* backup
* ideally *continuous*

**Godzilla destroyed my office!!!!!!**
* offsite (cloud)

***

<img src="images/godzilla.jpg" width="400" />


You can't reproduce
========================================================

...what you've lost. What if you need access to a file as it existed 1, 10, or 100, or 1000 days ago?
- Incremental backups (minimum)
- Version control (better). A *repository* holds files and tracks changes: what, by whom, why

***

<img src="images/tardis.jpg" width="400" />


Version control
========================================================

**Git** (and website **GitHub**) are the most popular version control tools for use with R, and many other languages:
- version control
- sharing code with collaborators in a *repository*
- issue tracking
- public or private

***

<img src="images/github.png" width="400" />


Reproducible research example
========================================================

A typical project/paper directory for me:
```
1-download.R
2-process_data.R
3-analyze_data.R
4-make_graphs.R
logfiles/
processed_data/
rawdata/
outputs/
```

This directory contains (and perhaps other) *scripts* that are backed up both *locally* and *remotely*. It is under *version control*, so it's easy to track changes, by multiple people and over time.


Reproducible research example
========================================================

A typical project/paper directory for me:
```
1-download.R
2-process_data.R
3-analyze_data.R
4-make_graphs.R
logfiles/
processed_data/
rawdata/
outputs/
```

For me almost any project starts from my [default script](https://github.com/bpbond/R_analysis_script).


Reproducible research example
========================================================

<img src="images/repository.png" width="600" />

From [this paper of mine](http://iopscience.iop.org/article/10.1088/1748-9326/11/8/084004/meta).


Hands-on: setting up R and RStudio
========================================================
type: prompt
incremental: false

If you're doing the exercises and problems, you'll need these
packages:
- `dplyr` - fast, flexible tool for working with data frames
- `tidyr` - reshaping and cleaning data
- `ggplot2` - popular package for visualizing data

We'll also use this data package:
- `babynames` - names provided to the SSA 1880-2013


Getting ready to process data in R
========================================================
type: section


Things you should know: basics
========================================================

This workshop assumes you understand the basics of R:

- What R is, how to start and quit it
- How to get help, both in R and out

```{r, eval=FALSE}
# Get help for the `summary` function
?summary

# Looks for a topic
apropos("GLM")

# Get help for an entire package
help(package = 'tidyr')
```


TYSK: vectors and data frames
========================================================

- A `data.frame` is what we'll mostly use today
- Under the hood, it's actually a `list` of `vector` structures:

```{r}
str(cars)
```

- Lists are also extremely useful - learn how to use them efficiently (e.g. `lapply`)


TYSK: NA and friends
========================================================

One of R's strengths is that missing values are a first-class data type: `NA`.

```{r}
x <- c(1:3, NA, 5)
# Which are NA?
is.na(x)
any(is.na(x))
```

***

```{r}
which(is.na(x))
x[!is.na(x)]
```

It's also useful to be familiar with `is.infinite` and `is.nan`.


TYSK: vectorization
========================================================

- *Vectorised operations* operate on a vector or data frame all at once. One of the simplest:

```{r}
myvector <- 1:5
myvector * 2
```

This is **really important**. You want to take advantage of R's capabilities here, and avoid labor-intensive `for` loops.

```{r}
for(i in seq_along(myvector)) {
  myvector[i] <- myvector[i] * 2
}
```


TYSK: packages
========================================================

- *Packages* are pieces of software that can be loaded into R. There are thousands, for all kinds of tasks and needs.

```{r, eval=FALSE}
library(ggplot2)
# qplot: a "quick plot" function in ggplot2
qplot(speed, dist, data = cars)
```

You'll also see the double-colon notation, `ggplot2::qplot`, which denotes accessing the _exported symbol_ of a _package_ (technically, a namespace).
***

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
qplot(speed, dist, data=cars)
```


TYSK: is R the right tool for the job?
========================================================

R is great, but it might not be the right (or at least only) thing you want. There are other tools that might be better for your specific need!
- Python, C++, Hadoop, CDO/NCL, bash, ...

**CDO** is a good example here: we spent a while developing the [RCMIP5](https://cran.r-project.org/web/packages/RCMIP5/index.html) R package for processing CMIP5 data, only to discover the already-existing [CDO](https://code.zmaw.de/projects/cdo), which is more robust, much more capable, and _far_ faster.

It's almost always a good idea to do some research!


Hands-on: Examining `iris` in base R
========================================================
type: prompt
incremental: false

* Print a summary of the built-in `iris` data. How many rows and columns does it have?
* What's the maximum `Petal.Length` value? What row is it in? (See `which.max`.)
* Pull out (using base R's `subset`) all the observations with `Petal.Length` greater than 6. How many observations are there?
* Remove colums 1 and 3 from `iris`. What are the names of the remaining columns? 
* Look at the underlying structure of `iris` using `str()` and `class()`.


Reshaping and tidying data
========================================================
type: section


History lesson
========================================================

<img src="images/history.png" width="850" />


'Tidy' data
========================================================

We're going to use `tidyr` today, the successor to the popular `reshape2` package. It focuses on tools to clean and _tidy_ data.

>Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.

http://vita.had.co.nz/papers/tidy-data.pdf


'Tidy' data
========================================================

We _clean_ and _reshape_ data to get it from an untidy form (above) to a tidy one (below).

```{r, echo=FALSE}
df <- data.frame(` ` = c("John Smith", "Jane Doe", "Mary Johnson"), `Treatment A` = c("--", 16, 3), `Treatment B` = c(2, 11, 1), check.names = FALSE)
kable(df)
data.frame(name = c("John Smith", "Jane Doe", "Mary Johnson", "John Smith", "Jane Doe", "Mary Johnson"), treatment = c("a", "a", "a", "b", "b", "b"), result = c(NA, 16, 3, 2, 11, 1))
```


Brief detour: pipelines in R
========================================================

`dplyr` and `tidyr` both *import* the [magrittr](https://github.com/smbache/magrittr) package, which introduces a **pipeline** operator `%>%` to R.

Not everyone is a fan of piping, and there are situations where it's not appropriate; but we'll follow `dplyr` convention and use it frequently.


`magrittr` pipelines in R
========================================================

`%>%` _pipes_ the output of one command into the input of the next. Standard R notation:

```{r, eval=FALSE}
nrow(iris)
```

Notation using pipes:

```{r, eval=FALSE}
library(magrittr)
iris %>% nrow
```

By default the output goes in as the _first_ parameter of the next function, but you can change this.


`magrittr` pipelines in R
========================================================

Standard R notation:

```{r, eval=FALSE}
x <- read_my_data(f)
y <- merge_data(clean_data(x), otherdata)
z <- summarize_data(y)
```

Pipeline notation:

```{r, eval=FALSE}
read_my_data(f) %>%
  clean_data %>%
  merge_data(otherdata) %>%
  summarize_data ->
  z
```


Reshaping datasets
========================================================

Now that we understand pipes, let's look at reshaping data. This is important, because often data aren't in the form (_tidy_ or not) you want. 

The `head()` of the `iris` dataset:
```{r, echo=FALSE}
head(iris[c(1, 4, 5)])
```

Discuss: why is this not a great form for the data?

Reshaping datasets
========================================================

The `tidyr::gather` function 'gathers' the data, taking multiple columns and collapsing them into key-value pairs.

It's very similar in function, though not in syntax, to the older `reshape2::melt` command that makes 'molten' data.


Reshaping datasets
========================================================

Remember the form of `iris`:

```{r}
str(iris)
```


Reshaping datasets
========================================================

We call `gather`, telling it that we want to 'gather' everything *except* for `Species`, and we want the resulting columns to be named `variable` and `value`:

```{r}
library(tidyr)
iris %>% 
  gather(variable, value_cm, -Species)
```


Reshaping datasets
========================================================

We call `gather`, telling it that we want to 'gather' everything *except* for `species`, and we want the resulting columns to be named `variable` and `value`:

```{r, echo=FALSE}
library(tidyr)
iris %>% 
  gather(variable, value_cm, -Species) %>% head
```

What's the problem here, though?


Reshaping datasets
========================================================

```{r}
iris %>% 
  gather(variable, value_cm, -Species) %>% 
  separate(variable, 
           into = c("part", "dimension"))
``` 


Reshaping datasets
========================================================

You can also work the opposite way, spreading data _across columns_.

The `tidyr::spread` functions is actually considerably less functional than `reshape2::cast`. This is by design, but it's also frustrating.


Reshaping datasets
========================================================

```{r}
df <- data.frame(x = c("a", "b"), 
                 y = c(3, 4), 
                 z = c(5, 6))
df
df %>% spread(x, y)
```


Reshaping datasets
========================================================

```{r}
df
df %>% 
  spread(x, y) %>% 
  gather(x, y, a:b, na.rm = TRUE)
```


Merging datasets
========================================================

Often, as we clean and reshape data, we want to merge different datasets together. The built-in `merge` command does this well.

Let's say we have a data frame containing information on how pretty each of the `iris` species is:

```{r}
howpretty <- data.frame(
  Species = unique(iris$Species), 
  pretty = c("ugly", "ok", "lovely"))
howpretty
```


Merging datasets
========================================================

`merge` looks for names in common between two data frames, and uses these to merge.

```{r, eval=FALSE}
merge(iris, howpretty)
```

```{r, echo=FALSE}
head(merge(iris, howpretty)[c(1, 2, 6)])
```

(Viewing only a few columns and rows.) The `dplyr` package has more varied, faster database-style join operations.


Hands-on: Reshaping `pew`
========================================================
type: prompt
incremental: false

```{r}
pew <- read.table(
  file = "http://stat405.had.co.nz/data/pew.txt",
  header = TRUE,
  stringsAsFactors = FALSE,
  check.names = FALSE)
pew[1:3]
```


Hands-on: Reshaping `pew`
========================================================
type: prompt
incremental: false

* Reshape `pew` to a 'tidy' format, with column names "religion", "income", and "count".

* Reshape the tidy `pew` into a form with the different religions as column headers (variables) and income as rows (observations).

```{r, echo=FALSE}
pew %>%
  gather(income, count, -religion) ->
  tidypew
tidypew$religion <- substr(tidypew$religion, 1, 10)  # for clean display
tidypew %>%
  spread(religion, count) ->
  pew_wide
```


Hands-on: Reshaping `pew`
========================================================
type: prompt
incremental: false

```{r, eval=FALSE}
pew %>%
  gather(income, count, -religion)
```

```{r, echo=FALSE}
tidypew[1:3]
```


Hands-on: Reshaping `pew`
========================================================
type: prompt
incremental: false

```{r, eval=FALSE}
pew %>%
  gather(income, count, -religion) %>%
  spread(religion, count)
```

```{r, echo=FALSE}
pew_wide[1:3]
```


More on tidying data
========================================================

Sometimes a column encodes multiple pieces of information, and we want to split it. For example, say we have a `data.frame` of CMIP5 filenames:

```{r, echo=FALSE}
cmip5 <- data.frame(filename = c("dissic_Oyr_HadGEM2-ES_rcp85_r1i1p1_2100-2107.nc", "npp_Lmon_CESM1-BGC_historical_r1i1p1_185001-200512.nc", "npp_Lmon_HadGEM2-ES_rcp45_r3i1p1_208012-210011.nc"))
```

```{r, echo=FALSE}
cmip5
```


More on tidying data
========================================================

We can use `tidyr::separate` to split this into individual columns:

```{r, eval=FALSE}
cmip5 %>%
  separate(filename,
           into = c("var", "dom", 
                    "model","scenario", 
                    "ens", "yrs"), 
           sep = "_")
```

```{r, echo=FALSE}
cmip5 %>%
  separate(filename,
           into = c("var", "dom", 
                    "model","scenario", 
                    "ens", "yrs"), 
           sep = "_") -> cmip5
cmip5[1:5]
```


More on tidying data
========================================================

`tidyr` functions can also:

* complete data (i.e., fill in missing combinations)
* separate multiple values into different rows
```{r echo=FALSE}
df <- data.frame(
  x = 1:3,
  y = c("a", "d,e,f", "g,h"),
  z = c("1", "2,3,4", "5,6"),
  stringsAsFactors = FALSE
)
df
```

```{r}
separate_rows(df, y, z, convert = TRUE)
```


Summarizing data
========================================================
type: section


Summarizing and manipulating data
========================================================

We often want to summarize data by groups as an intermediate or final step. For example, for each subgroup we might want to:

* Compute mean, max, min, etc. (`n`->1)
* Compute rolling mean, rank, etc. (window functions, `n`->`n`)
* Fit models and extract their parameters, goodness of fit, etc.

Specific examples:

* `cars`: for each speed, what's the farthest distance traveled?
* `iris`: how many samples were taken from each species?
* `babynames`: what's the most common name over time?


Split-apply-combine
========================================================

These are generally known as *split-apply-combine* problems.

<img src="images/split_apply_combine.png" width="600" />

From https://github.com/ramnathv/rblocks/issues/8


aggregate
========================================================

Base R has an `aggregate` function. It's not particularly fast or flexible, and confusingly it has different forms (syntax).

It can however be useful for simple operations:

```{r}
# What's the farthest distance at each speed?
aggregate(dist ~ speed, 
          data = cars, FUN = max)
```


dplyr
========================================================

The newer `dplyr` package specializes in data frames, recognizing that most people use them most of the time.

`dplyr` also allows you to work with remote, out-of-memory databases, using exactly the same tools, because it abstracts away *how* your data is stored.

`dplyr` is flexible and **extremely fast**.


Verbs
========================================================

`dplyr` provides functions for each basic *verb* of data manipulation. These almost all have analogues in base R, but use a consistent, compact syntax, and are very high performance.

The most important basic ones are:

* `filter()` - select rows; like `base::subset()`
* `arrange()` - reorder rows; like `order()`
* `select()` - select columns
* `mutate()` - add new columns
* `summarise()` - like `aggregate`


Grouping
========================================================

`dplyr` verbs become particularly powerful when used in conjunction with *groups* we define in the dataset.

```{r}
library(dplyr)
cars %>%
  group_by(speed)
```


Summarizing cars
========================================================

We previously did this using `aggregate`. Now, `dplyr`:

```{r}
cars %>% 
  group_by(speed) %>% 
  summarise(max(dist))
```


Summarizing iris
========================================================

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise(msl = mean(Sepal.Length))
```


Summarizing iris
========================================================

We can apply (multiple) functions across (multiple) columns.

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise_all(mean)
```


Summarizing iris
========================================================

We can apply (multiple) functions across (multiple) columns.

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise_each(funs(mean, median, sd), 
                 Sepal.Length)
```


Introducting `babynames`
========================================================

```{r}
library(babynames)
babynames
```


Summarizing babynames
========================================================

Describe the individual steps here, and try them yourself:

```{r}
babynames %>%
  group_by(year, sex) %>% 
  summarise(n = n()) %>%
  spread(sex, n)
```


Summarizing babynames: speed
========================================================

```{r, cache=TRUE}
system.time(x <- babynames %>% group_by(year) %>% summarise(n()))
system.time(x <- aggregate(name ~ year, data = babynames, FUN = length))
```


========================================================

What does this calculate?

```{r}
babynames %>%
  group_by(year, sex) %>% 
  summarise(prop = max(prop), 
            name = name[which.max(prop)])
```


Summarizing babynames
========================================================

<img src="images/popular_babynames.png" width="800" />

https://en.wikipedia.org/wiki/Linda_(1946_song)


Summarizing babynames
========================================================

The `dplyr` package thus allows us to clearly and compactly specify a series of data-processing operations.

```{r, eval=FALSE}
my_data %>%
  # Prepare for processing
  filter(by some condition) %>%
  group_by(groups) %>%
  
  # Summarize / compute
  summarise(variables of interest) %>%
  mutate(new_column = ...) %>%
  
  # Format for output
  arrange(sort_variables) %>%
  select(-columns_to_remove) ->
  output_data
```


Hands-on: `babynames`
========================================================
type: prompt
incremental: false

* Load the dataset using `library(babynames)`.
* Read its help page. Look at its structure (rows, columns, summary). How many unique names are there in the database?


Hands-on: `babynames`
========================================================
type: prompt
incremental: false

* Use `dplyr` to calculate the total number of babies registered in the SSA database for each year and sex. This will involve a `group_by` step and then a `summarise` step.

```{r, echo=FALSE}
babynames %>%
  group_by(year, sex) %>% 
  summarise(n=sum(n)) %>% 
  qplot(year, n, data = ., color = sex)
```


Hands-on: `babynames`
========================================================
type: prompt
incremental: false

* Does your name occur in the SSA database? Plot the number of entries for it (or any name) name over time. This will involve a `filter` step, a `group_by` step, and a `summarise` step (and finally a `ggplot2::qplot`).

```{r, eval=FALSE}
babynames %>% 
  ... %>%
  qplot(year, prop, data = ., color = sex)
```


Hands-on: `babynames`
========================================================
type: prompt
incremental: false

* Trickier: use a _window function_ (n -> n) to compute the rank of your name over time.

```{r}
babynames %>% 
  group_by(year) %>% 
  mutate(rank = min_rank(desc(prop))) %>% 
  filter(name == "Benjamin", sex == "M") %>% 
  qplot(year, rank, data = .)
```


Hands-on: `babynames`
========================================================
type: prompt
incremental: false

* What's happening here?

```{r, eval=FALSE}
babynames %>% 
  group_by(name) %>%
  summarise(totaln = sum(n)) %>%
  mutate(rnk = 
           min_rank(desc(totaln))) %>%
  filter(rnk <= 20) ->
  top20

babynames%>%
  filter(name %in% top20$name) %>%
  qplot(year, prop, data = ., color = name) +
    facet_wrap(~name)
```


Hands-on: `babynames`
========================================================
type: prompt
incremental: false

```{r, cache=TRUE, echo=FALSE}
babynames %>% 
  group_by(sex, name) %>%
  summarise(totaln = sum(n)) %>%
  mutate(rnk = 
           min_rank(desc(totaln))) %>%
  filter(rnk <= 20) ->
  top20

babynames%>%
  filter(name %in% top20$name) %>%
  qplot(year, prop, data = ., color = sex) +
  facet_wrap(~name)
```


Final notes on data manipulation
========================================================

Remember that everything you can do in `tidyr` and `dplyr` you can do in base R! But often slower, and less clearly (imho).

Things `dplyr` is **not** good for:

* processing arrays, matrices, and (mostly) lists - i.e., anything that's not a data frame
* simple/small tasks - `aggregate` may be faster in some cases


Things we didn't talk much about
========================================================

- reading data into R: reading from spreadsheets (`readxl`), reading large datasets
- working with non-text data: binary, netcdf, spatial
- lists, matrices, and arrays
- graphing data
- parallelization or performance


Last thoughts
========================================================

>The best thing about R is that it was written by statisticians. The worst thing about R is that it was written by statisticians.
>
>-- Bow Cowgill

All the source code for this presentation is available at https://github.com/bpbond/R-data-picarro


Resources
========================================================
type: section


Resources
========================================================

* [CRAN](http://cran.r-project.org) - The Comprehensive R Archive Network.
* [GitHub](https://github.com/JGCRI) - The JGCRI organization page on GitHub.
* [RStudio](http://www.rstudio.com) - the integrated development environment for R. Makes many things hugely easier.
* [Advanced R](http://adv-r.had.co.nz) - the companion website for “Advanced R”, a book in Chapman & Hall’s R Series. Detailed, in depth look at many of the issues covered here.


Resources
========================================================

R has many contributed *packages* across a wide variety of scientific fields. Almost anything you want to do will have packages to support it.

[CRAN](http://cran.r-project.org) also provides "Task Views". For example:

***

- Bayesian
- Clinical Trials
- Differential Equations
- Finance
- Genetics
- HPC
- Meta-analysis
- Optimization
- [**Reproducible Research**](http://cran.r-project.org/web/views/ReproducibleResearch.html)
- Spatial Statistics
- Time Series
