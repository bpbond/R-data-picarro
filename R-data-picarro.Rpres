MORE reproducible data analysis using R
========================================================
author: Ben Bond-Lamberty
date: August 2016
font-family: 'Helvetica'

A workshop covering reproducibility and repository design; data cleaning and reshaping; and using the `dplyr` package for computation.

JGCRI


The plan
========================================================

* Introduction: reproducible research and repository design (45 minutes; hands-on: installing the packages we'll need)
* Examining and cleaning data (45 minutes; hands-on: the `iris` and `Pew` datasets, CMIP5 example)
* Summarizing and manipulating data (45 minutes; hands-on: the `babynames` dataset)

**This workshop assumes you understand the basics of R.**

Feedback: <a href="mailto:bondlamberty@pnnl">bondlamberty@pnnl.gov</a> or  [@BenBondLamberty](https://twitter.com/BenBondLamberty).


Is R the right tool for the job?
========================================================

R is great, but it might not be the right (or at least only) thing you want. There are other tools that might be better for your specific need!
- Python, C++, Hadoop, CDO/NCL, bash, ...


Reproducibility and repositories
========================================================
type: section


Reproducibility
========================================================

We are in the era of collaborative 'big data', but even if you work by yourself with 'little data' you have to have some skills to deal with those data, now and in the future.

>Your most important collaborator is your future self. It’s important to make a workflow that you can use time and time again, and even pass on to others in such a way that you don’t have to be there to walk them through it. [Source](http://berkeleysciencereview.com/reproducible-collaborative-data-science/)

Reproducibility generally means *scripts* or *programs* tied to *open source software*.


Reproducibility
========================================================

...is not all or nothing. A great, succinct resource here is Karl Broman's "initial steps toward reproducible research" webpage:

http://kbroman.org/steps2rr/

>Organizing analyses so that they are reproducible is not easy. It requires diligence and a considerable investment of time: to learn new computational tools, and to organize and document analyses as you go.

>But partially reproducible is better than not at all reproducible. Just try to make your next paper or project better organized than the last.


You can't reproduce
========================================================
...what doesn't exist.

**Gozilla ate my computer!**
* *automated* backup
* ideally *continuous*

**Godzilla destroyed my office!!!!!!**
* offsite (cloud)

***

<img src="images/godzilla.jpg" width="400" />


You can't reproduce
========================================================

...what you've lost. What if you need access to a file as it existed 1, 10, or 100, or 1000 days ago?
- Incremental backups (minimum)
- Version control (better). A *repository* holds files and tracks changes: what, by whom, why

***

<img src="images/tardis.jpg" width="400" />


Version control
========================================================

**Git** (and website **GitHub**) are the most popular version control tools for use with R, and many other languages:
- version control
- sharing code with collaborators in a *repository*
- issue tracking
- public or private

***

<img src="images/github.png" width="400" />


Reproducible research example
========================================================

A typical project/paper directory for me:
```
1-download.R
2-process_data.R
3-analyze_data.R
4-make_graphs.R
logfiles/
processed_data/
rawdata/
outputs/
```

This directory contains (and perhaps other) *scripts* that are backed up both *locally* and *remotely*. It is under *version control*, so it's easy to track changes, by multiple people and over time.


Reproducible research example
========================================================

A typical project/paper directory for me:
```
1-download.R
2-process_data.R
3-analyze_data.R
4-make_graphs.R
logfiles/
processed_data/
rawdata/
outputs/
```

For me almost any project starts from my [default script](https://github.com/bpbond/R_analysis_script).


Reproducible research example
========================================================

<img src="images/repository.png" />

From [this paper of mine](http://iopscience.iop.org/article/10.1088/1748-9326/11/8/084004/meta).

Hands-on: setting up R and RStudio
========================================================
type: prompt
incremental: false

If you're doing the exercises and problems, you'll need these
packages:
- `dplyr` - fast, flexible tool for working with data frames
- `tidyr` - reshaping and cleaning data
- `ggplot2` - popular package for visualizing data

We'll also use this data package:
- `babynames` - names provided to the SSA 1880-2013

Finally, we'll download a [repository](https://github.com/bpbond/R-data-picarro) (collection of code and data) for this workshop.

Let's do that now.


R basics
========================================================
type: section


Things you should know: basics
========================================================

This workshop assumes you understand a few basics of R:

- What R is
- How to start and quit it
- How to get help

```{r, eval=FALSE}
# This is a comment
# Get help for the `summary` function
?summary

# Get help for an entire package
help(package = 'ggplot2')
```


Things you should know: vectors
========================================================

- The *vector* data type

```{r}
myvector <- 1:5
myvector <- c(1, 2, 3, 4, 5)
myvector <- seq(1, 5)
myvector
myvector[2:3]
```

***

```{r}
sum(myvector)
```

*Vectorised operations* operate on a vector all at once:

```{r}
myvector * 2
```


Things you should know: scripts
========================================================

The difference between a *script* (stored program) and *command line* (immediate response).

In general, you want to use scripts, which provide *reproducibility*.


```{r, eval = FALSE}
source("myscript.R")
```

***

<img src="images/mayan_script.gif" />


Things you should know: packages
========================================================

- *Packages* are pieces of software that can be loaded into R. There are thousands, for all kinds of tasks and needs.

```{r, eval=FALSE}
# The single most popular R package is `ggplot2`
library(ggplot2)

# qplot:
# a "quick plot" function in ggplot2
qplot(speed, 
      dist, 
      data = cars)
```

***

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
qplot(speed, dist, data=cars)
```


Hands-on: examining the `iris` dataset
========================================================
type: prompt
incremental: false

Hands-on work in RStudio.
* Built-in datasets
* Using `summary`, `names`, `head`, `tail`
* Looking at particular rows and columns
* Subsetting the data
* Basic plots of the data


Computing on columns
========================================================

This can be simple...

```{r}
d <- data.frame(x = 1:3)
d$y <- d$x * 2
d$z <- cumsum(d$y) # cumulative sum
d$four <- ifelse(d$y == 4, "four", "not 4") 
d
```



Understanding and dealing with NA
========================================================

One of R's strengths is that missing values are a first-class data type: `NA`.

```{r}
x <- c(1, 2, 3, NA)
# Which are NA?
is.na(x)
any(is.na(x))
```

***

```{r}
which(is.na(x))
x[!is.na(x)]
```


Understanding and dealing with NA
========================================================

Like `NaN` and `Inf`, generally `NA` 'poisons' operations, so NA values must be explicitly ignored and/or removed.

```{r}
x <- c(1, 2, 3, NA)
sum(x) # NA
sum(x, na.rm = TRUE)
```


Dealing with dates
========================================================

R has a `Date` class representing calendar dates, and an `as.Date` function for converting to Dates. The `lubridate` package is often useful (and easier) for these cases:

```{r}
library(lubridate)
x <- c("09-01-01", "09-01-02") # character!
ymd(x)   # there's also dmy, ymd_hms, etc.
```

Once data are in `Date` format, the time interval between them can be computed simply by subtraction. See `?difftime`


Merging datasets
========================================================

Often, as we clean and reshape data, we want to merge different datasets together. The built-in `merge` command does this well.

Let's say we have a data frame containing information on how pretty each of the `iris` species is:

```{r, echo=FALSE}
howpretty <- data.frame(Species = unique(iris$Species), 
                        pretty = c("ugly", "ok", "lovely"))
howpretty
```


Merging datasets
========================================================

`merge` looks for names in common between two data frames, and uses these to merge.

```{r, eval=FALSE}
merge(iris, howpretty)
```

```{r, echo=FALSE}
head(merge(iris, howpretty)[c(1, 2, 6)])
```

(Viewing only a few columns and rows.) The `dplyr` package has more varied, faster database-style join operations.


Summarizing and manipulating data
========================================================
type: section


History lesson
========================================================

<img src="images/history.png" width="850" />


Operation pipelines in R
========================================================

`dplyr` and `tidyr` both *import* the [magrittr](https://github.com/smbache/magrittr) package, which introduces a **pipeline** operator `%>%` to R.

Not everyone is a fan of piping, and there are situations where it's not appropriate; but we'll use it frequently.


`magrittr` pipelines in R
========================================================

Standard R notation:

```{r, eval=FALSE}
x <- read_my_data(f)
y <- merge_data(clean_data(x), otherdata)
z <- summarize_data(y)
```

Pipeline notation:

```{r, eval=FALSE}
read_my_data(f) %>%
  clean_data %>%
  merge_data(otherdata) %>%
  summarize_data ->
  z
```


Reshaping datasets
========================================================

This is a **CRITICALLY** important skill, because often data aren't in the form you want. Let's look at the `head()` of the `iris` dataset:
```{r, echo=FALSE}
head(iris[1:3])
```

Discuss: why is this not a great form for the data?

Reshaping datasets
========================================================

The `tidyr::gather` function 'gathers' the data, taking multiple columns and collapsing them into key-value pairs.


Reshaping datasets
========================================================

Remember the form of `iris`:

```{r, echo=FALSE}
head(iris[1:3])
```


Reshaping datasets
========================================================

We call `gather`, telling it that we want to 'gather' everything *except* for `species`, and we want the resulting columns to be named `variable` and `value`:

```{r}
library(tidyr)
iris %>% 
  gather(variable, value, -Species)
```


Reshaping datasets
========================================================

We call `gather`, telling it that we want to 'gather' everything *except* for `species`, and we want the resulting columns to be named `variable` and `value`:

```{r, echo=FALSE}
library(tidyr)
iris %>% 
  gather(variable, value, -Species) %>% head
```

What's the problem here, though?


Reshaping datasets
========================================================

```{r}
iris %>% 
  gather(variable, value, -Species) %>% 
  separate(variable, 
           into = c("part", "dimension"))
``` 


Reshaping datasets
========================================================

You can also work the opposite way, spreading data _across columns_.

The `tidyr::spread` functions is actually considerably less functional than `reshape2::cast`. This is by design, but it's also frustrating.

Reshaping datasets
========================================================

```{r}
df <- data.frame(x = c("a", "b"), 
                 y = c(3, 4), 
                 z = c(5, 6))
df
df %>% spread(x, y)
```

Reshaping datasets
========================================================

```{r}
df
df %>% 
  spread(x, y) %>% 
  gather(x, y, a:b, na.rm = TRUE)
```


Summarizing and manipulating data
========================================================

Thinking back to the typical data pipeline, we often want to summarize data by groups as an intermediate or final step. For example, for each subgroup we might want to:

* Compute mean, max, min, etc. (`n`->1)
* Compute rolling mean and other window functions (`n`->`n`)
* Fit models and extract their parameters, goodness of fit, etc.

Specific examples:

* `cars`: for each speed, what's the farthest distance traveled?
* `iris`: how many samples were taken from each species?
* `babynames`: what's the most common name over time?


Split-apply-combine
========================================================

These are generally known as *split-apply-combine* problems.

<img src="images/split_apply_combine.png" width="600" />

From https://github.com/ramnathv/rblocks/issues/8


aggregate
========================================================

Base R has an `aggregate` function. It's not particularly fast or flexible, and confusingly it has different forms (syntax).

It can however be useful for simple operations:

```{r}
# What's the farthest distance at each speed?
aggregate(dist ~ speed, 
          data = cars, FUN = max)
```


dplyr
========================================================

The newer `dplyr` package specializes in data frames, recognizing that most people use them most of the time.

`dplyr` also allows you to work with remote, out-of-memory databases, using exactly the same tools, because it abstracts away *how* your data is stored.

`dplyr` is **extremely fast**.


Verbs
========================================================

`dplyr` provides functions for each basic *verb* of data manipulation. These tend to have analogues in base R, but use a consistent, compact syntax, and are very high performance.

* `filter()` - subset rows; like `base::subset()`
* `arrange()` - reorder rows; like `order()`
* `select()` - select columns
* `mutate()` - add new columns
* `summarise()` - like `aggregate`


Grouping
========================================================

`dplyr` verbs become particularly powerful when used in conjunction with *groups* we define in the dataset. The `group_by` function converts an existing data frame into a grouped `tbl`.

```{r}
library(dplyr)
cars %>%
  group_by(speed)
```


Summarizing cars
========================================================

We previously did this using `aggregate`. Now, `dplyr`:

```{r}
cars %>% 
  group_by(speed) %>% 
  summarise(max(dist))
```


Summarizing iris
========================================================

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise(msl = mean(Sepal.Length))
```


Summarizing iris
========================================================

We can apply (multiple) functions across (multiple) columns.

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise_each(funs(mean, median, sd), 
                 Sepal.Length)
```


Introducting `babynames`
========================================================

```{r}
library(babynames)
babynames
```


Summarizing babynames
========================================================

What does this calculate?

```{r}
babynames %>%
  group_by(year, sex) %>% 
  summarise(prop = max(prop), 
            name = name[which.max(prop)])
```


Summarizing babynames
========================================================

<img src="images/popular_babynames.png" width="800" />

https://en.wikipedia.org/wiki/Linda_(1946_song)


Why use dplyr?
========================================================

* Clean, concise, and consistent syntax.

* In general `dplyr` is ~10x faster than the older `plyr` package. (And `plyr` was ~10x faster than base R.)

* Same code can work with data frames or remote databases.


Hands-on: manipulating the `babynames` dataset
========================================================
type: prompt
incremental: false

Load the dataset using `library(babynames)`.

Read its help page. Look at its structure (rows, columns, summary).

Use `dplyr` to calculate the total number of names in the SSA database for each year. 

Calculate the 5th most popular name for girls in each year. Hint: `nth()`.




Things we didn't talk about
========================================================

- reading data into R (not much)
- working with non-text data
- reshaping data
- writing data
- graphing data



Last thoughts
========================================================

>The best thing about R is that it was written by statisticians. The worst thing about R is that it was written by statisticians.
>
>-- Bow Cowgill

All the source code for this presentation is available at https://github.com/bpbond/R-data-picarro


Resources
========================================================
type: section


Resources
========================================================

* [CRAN](http://cran.r-project.org) - The Comprehensive R Archive Network.
* [GitHub](https://github.com/JGCRI) - The JGCRI organization page on GitHub.
* [RStudio](http://www.rstudio.com) - the integrated development environment for R. Makes many things hugely easier.
* [Advanced R](http://adv-r.had.co.nz) - the companion website for “Advanced R”, a book in Chapman & Hall’s R Series. Detailed, in depth look at many of the issues covered here.


Resources
========================================================

R has many contributed *packages* across a wide variety of scientific fields. Almost anything you want to do will have packages to support it.

[CRAN](http://cran.r-project.org) also provides "Task Views". For example:

***

- Bayesian
- Clinical Trials
- Differential Equations
- Finance
- Genetics
- HPC
- Meta-analysis
- Optimization
- [**Reproducible Research**](http://cran.r-project.org/web/views/ReproducibleResearch.html)
- Spatial Statistics
- Time Series
